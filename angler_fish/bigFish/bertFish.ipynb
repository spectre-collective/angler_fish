{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3234\n",
      "Return-Path: <james_ngola2002@maktoob.com>\n",
      "X-Sieve: cmu-sieve 2.0\n",
      "Return-Path: <james_ngola2002@maktoob.com>\n",
      "Message-Id: <200210310241.g9V2fNm6028281@cs.CU>\n",
      "From: \"MR. JAMES NGOLA.\" <james_ngola2002@maktoob.com>\n",
      "Reply-To: james_ngola2002@maktoob.com\n",
      "To: webmaster@aclweb.org\n",
      "Date: Thu, 31 Oct 2002 02:38:20 +0000\n",
      "Subject: URGENT BUSINESS ASSISTANCE AND PARTNERSHIP\n",
      "X-Mailer: Microsoft Outlook Express 5.00.2919.6900 DM\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=\"us-ascii\"\n",
      "Content-Transfer-Encoding: 8bit\n",
      "X-MIME-Autoconverted: from quoted-printable to 8bit by sideshowmel.si.UM id g9V2foW24311\n",
      "Status: O\n",
      "\n",
      "FROM:MR. JAMES NGOLA.\n",
      "CONFIDENTIAL TEL: 233-27-587908.\n",
      "E-MAIL: (james_ngola2002@maktoob.com).\n",
      "\n",
      "URGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\n",
      "\n",
      "\n",
      "DEAR FRIEND,\n",
      "\n",
      "I AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY GUARD ON 16TH JAN. 2001.\n",
      "\n",
      "\n",
      "THE INCIDENT OCCURRED IN OUR PRESENCE WHILE WE WERE HOLDING MEETING WITH HIS EXCELLENCY OVER THE FINANCIAL RETURNS FROM THE DIAMOND SALES IN THE AREAS CONTROLLED BY (D.R.C.) DEMOCRATIC REPUBLIC OF CONGO FORCES AND THEIR FOREIGN ALLIES ANGOLA AND ZIMBABWE, HAVING RECEIVED THE PREVIOUS DAY (USD$100M) ONE HUNDRED MILLION UNITED STATES DOLLARS, CASH IN THREE DIPLOMATIC BOXES ROUTED THROUGH ZIMBABWE.\n",
      "\n",
      "MY PURPOSE OF WRITING YOU THIS LETTER IS TO SOLICIT FOR YOUR ASSISTANCE AS TO BE A COVER TO THE FUND AND ALSO COLLABORATION IN MOVING THE SAID FUND INTO YOUR BANK ACCOUNT THE SUM OF (USD$25M) TWENTY FIVE MILLION UNITED STATES DOLLARS ONLY, WHICH I DEPOSITED WITH A SECURITY COMPANY IN GHANA, IN A DIPLOMATIC BOX AS GOLDS WORTH (USD$25M) TWENTY FIVE MILLION UNITED STATES DOLLARS ONLY FOR SAFE KEEPING IN A SECURITY VAULT FOR ANY FURTHER INVESTMENT PERHAPS IN YOUR COUNTRY. \n",
      "\n",
      "YOU WERE INTRODUCED TO ME BY A RELIABLE FRIEND OF MINE WHO IS A TRAVELLER,AND ALSO A MEMBER OF CHAMBER OF COMMERCE AS A RELIABLE AND TRUSTWORTHY PERSON WHOM I CAN RELY ON AS FOREIGN PARTNER, EVEN THOUGH THE NATURE OF THE TRANSACTION WAS NOT REVEALED TO HIM FOR SECURITY REASONS.\n",
      "\n",
      "\n",
      "THE (USD$25M) WAS PART OF A PROCEEDS FROM DIAMOND TRADE MEANT FOR THE LATE PRESIDENT LAURENT KABILA WHICH WAS DELIVERED THROUGH ZIMBABWE IN DIPLOMATIC BOXES. THE BOXES WERE KEPT UNDER MY CUSTODY BEFORE THE SAD EVENT THAT TOOK THE LIFE OF (MR. PRESIDENT).THE CONFUSION THAT ENSUED AFTER THE ASSASSINATION AND THE SPORADIC SHOOTING AMONG THE FACTIONS, I HAVE TO RUN AWAY FROM THE COUNTRY FOR MY DEAR LIFE AS I AM NOT A SOLDIER BUT A CIVIL SERVANT I CROSSED RIVER CONGO TO OTHER SIDE OF CONGO LIBREVILLE FROM THERE I MOVED TO THE THIRD COUNTRY GHANA WHERE I AM PRESENTLY TAKING REFUGE. \n",
      "\n",
      "AS A MATTER OF FACT, WHAT I URGENTLY NEEDED FROM YOU IS YOUR ASSISTANCE IN MOVING THIS MONEY INTO YOUR ACCOUNT IN YOUR COUNTRY FOR INVESTMENT WITHOUT RAISING EYEBROW. FOR YOUR ASSISTANCE I WILL GIVE YOU 20% OF THE TOTAL SUM AS YOUR OWN SHARE WHEN THE MONEY GETS TO YOUR ACCOUNT, WHILE 75% WILL BE FOR ME, OF WHICH WITH YOUR KIND ADVICE I HOPE TO INVEST IN PROFITABLE VENTURE IN YOUR COUNTRY IN OTHER TO SETTLE DOWN FOR MEANINGFUL LIFE, AS I AM TIRED OF LIVING IN A WAR ENVIRONMENT. \n",
      "\n",
      "THE REMAINING 5% WILL BE USED TO OFFSET ANY COST INCURRED IN THE CAUSE OF MOVING THE MONEY TO YOUR ACCOUNT. IF THE PROPOSAL IS ACCEPTABLE TO YOU PLEASE CONTACT ME IMMEDIATELY THROUGH THE ABOVE TELEPHONE AND E-MAIL, TO ENABLE ME ARRANGE FACE TO FACE MEETING WITH YOU IN GHANA FOR THE CLEARANCE OF THE FUNDS BEFORE TRANSFRING IT TO YOUR BANK ACCOUNT AS SEEING IS BELIEVING. \n",
      "\n",
      "FINALLY, IT IS IMPORTANT ALSO THAT I LET YOU UNDERSTAND THAT THERE IS NO RISK INVOLVED WHATSOEVER AS THE MONEY HAD NO RECORD IN KINSHASA FOR IT WAS MEANT FOR THE PERSONAL USE OF (MR. PRESIDEND ) BEFORE THE NEFARIOUS INCIDENT OCCURRED, AND ALSO I HAVE ALL THE NECESSARY DOCUMENTS AS REGARDS TO THE FUNDS INCLUDING THE (CERTIFICATE OF DEPOSIT), AS I AM THE DEPOSITOR OF THE CONSIGNMENT.\n",
      "\n",
      "\n",
      "LOOKING FORWARD TO YOUR URGENT RESPONSE.\n",
      "\n",
      "YOUR SINCERELY,\n",
      "\n",
      "MR. JAMES NGOLA. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4000\n",
      "4224\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "%run ./seg_fraud.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "%run ./seg_normal.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./fish_cleaner.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('fraud.txt', 'r')\n",
    "f2 = open('enron3.txt', 'r')\n",
    "\n",
    "corpus_one = [item for item in f1]\n",
    "corpus_two = [item for item in f2]\n",
    "\n",
    "fs = split_set(corpus_one)\n",
    "ns = split_enron_set(corpus_two)\n",
    "\n",
    "fraud_set = []\n",
    "norm_set = []\n",
    "\n",
    "for i in range(len(fs)):\n",
    "    fraud_set.append(super_clean(fs[i]))\n",
    "for i in range(len(ns)):\n",
    "    norm_set.append(super_clean(ns[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_screen(x):        #remove emails from datasets that are less than 512 tokens long. \n",
    "    z = []\n",
    "    for i in range(len(x)):\n",
    "        temp = x[i].split()\n",
    "        if len(temp) >= 512:\n",
    "            z.append(x[i])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of fraud set before 512 token screening: 500\n",
      "Length of normal set before 512 token screening: 500\n",
      "\n",
      "\n",
      "Length of fraud after before 512 token screening: 500\n",
      "Length of normal after before 512 token screening: 500\n"
     ]
    }
   ],
   "source": [
    "print('Length of fraud set before 512 token screening: {}'.format(len(fraud_set)))\n",
    "print('Length of normal set before 512 token screening: {}'.format(len(norm_set)))\n",
    "\n",
    "fraud_set = bert_screen(fraud_set)\n",
    "norm_set = bert_screen(norm_set)\n",
    "\n",
    "print('\\n')\n",
    "print('Length of fraud after before 512 token screening: {}'.format(len(fraud_set)))\n",
    "print('Length of normal after before 512 token screening: {}'.format(len(norm_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_set = fraud_set[:500]\n",
    "norm_set = norm_set[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(fraud, norm):\n",
    "    new_set = []\n",
    "    for item in fraud:\n",
    "        t = (item, 'fraud')\n",
    "        new_set.append(t)\n",
    "    for item in norm:\n",
    "        t = (item, 'norm')\n",
    "        new_set.append(t)\n",
    "    return new_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = label(fraud_set,norm_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0      1\n",
      "0                  FROM:MR. JAMES NGOLA.    URGEN...  fraud\n",
      "1               Dear Friend,  I am Mr. Ben Sulema...  fraud\n",
      "2   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
      "3   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
      "4   Dear sir,    It is with a heart full of hope ...  fraud\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = g\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fraud': 500, 'norm': 500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(df[1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: \"text\", 1: \"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FROM:MR. JAMES NGOLA.    URGEN...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear Friend,  I am Mr. Ben Sulema...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear sir,    It is with a heart full of hope ...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   type\n",
       "0                  FROM:MR. JAMES NGOLA.    URGEN...  fraud\n",
       "1               Dear Friend,  I am Mr. Ben Sulema...  fraud\n",
       "2   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
       "3   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
       "4   Dear sir,    It is with a heart full of hope ...  fraud"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = df[df['type'] == 'fraud'] \n",
    "df_norm = df[df['type'] == 'norm'] \n",
    "df_norm = df_norm.sample(n=len(df_fraud))\n",
    "df = df_norm.append(df_fraud)\n",
    "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.head(800)\n",
    "test_data = df.tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array(train_data['text'])\n",
    "x2 = np.array(train_data['type'])\n",
    "\n",
    "y1 = np.array(test_data['text'])\n",
    "y2 = np.array(test_data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = []\n",
    "t2 = []\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    t1.append({'text': x1[i], 'type': x2[i]})\n",
    "    \n",
    "for i in range(len(y1)):\n",
    "    t2.append({'text': y1[i], 'type': y2[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = t1\n",
    "test_data = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
    "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell above tokenizes text sequences of up to 512 tokens, as that is the max input size of BERT.\n",
    "Datapoints posessing less than 512 tokens are \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = []\n",
    "test_y = []\n",
    "t1 = []\n",
    "t2 = []\n",
    "\n",
    "for item in train_labels:\n",
    "    if str(item) == 'fraud': t1.append(1)\n",
    "    else: t1.append(0)    \n",
    "\n",
    "        \n",
    "for item in test_labels:\n",
    "    if str(item) == 'fraud': t2.append(1)\n",
    "    else: t2.append(0)        \n",
    "    \n",
    "train_y = np.array(t1)\n",
    "test_y = np.array(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DAtA LABEL VECTOR\n",
      "[0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
      " 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
      " 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0\n",
      " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
      " 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1\n",
      " 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1\n",
      " 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
      " 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0\n",
      " 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
      " 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
      " 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0]\n",
      "[1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING DAtA LABEL VECTOR\")\n",
    "print(train_y)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__() #DEFAULT CONSTRUCTOR INIT\n",
    "    \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased') #USE BERT BASE FOR LIGHTER LOAD\n",
    "        self.dropout = nn.Dropout(dropout) #DROPOUT = DEFAULT\n",
    "        self.linear = nn.Linear(768, 1)    #LINEAR ACTIVATION LAYER, HIDDEN VECTOR LENGTH 768\n",
    "        self.sigmoid = nn.Sigmoid()        #SIGMOID ACTIVATION LAYER, S SHAPE DECISION BOUNDARY\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba\n",
    "    #FUNCTION ABOVE DEFINES FLOW OF DATA FOR BERT MODEL.\n",
    "    #POOLED OUTPUT = DEFAULT BERT POOLED OUTPUT, ABSTRACTION OF DATAPOINT\n",
    "    #DROPOUT = DEFAULT BERT DROPOUT\n",
    "    #LINEAR_OUTPUT = LINEAR ACTIVATION WITH HIDDEN SIZE 768\n",
    "    #FINAL STEP, PUT RESULT INTO SIGMOID ACTIVATION FOR RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "#CREATE TENSORS FOR TRAINING. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=1)\n",
    "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATEMENT BELOW ITERATES THROUGH EACH ITEM IN THE TRAINING DATASET AT ONE EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/800.0 loss: 0.5651776790618896 \n",
      "Epoch:  1\n",
      "1/800.0 loss: 0.6311807930469513 \n",
      "Epoch:  1\n",
      "2/800.0 loss: 0.5909890929857889 \n",
      "Epoch:  1\n",
      "3/800.0 loss: 0.6638465076684952 \n",
      "Epoch:  1\n",
      "4/800.0 loss: 0.714335811138153 \n",
      "Epoch:  1\n",
      "5/800.0 loss: 0.7297720909118652 \n",
      "Epoch:  1\n",
      "6/800.0 loss: 0.7072210567338126 \n",
      "Epoch:  1\n",
      "7/800.0 loss: 0.7209253460168839 \n",
      "Epoch:  1\n",
      "8/800.0 loss: 0.7008258832825555 \n",
      "Epoch:  1\n",
      "9/800.0 loss: 0.6895004570484161 \n",
      "Epoch:  1\n",
      "10/800.0 loss: 0.6720695116303184 \n",
      "Epoch:  1\n",
      "11/800.0 loss: 0.6847949773073196 \n",
      "Epoch:  1\n",
      "12/800.0 loss: 0.6731671049044683 \n",
      "Epoch:  1\n",
      "13/800.0 loss: 0.6827818666185651 \n",
      "Epoch:  1\n",
      "14/800.0 loss: 0.6739884535471599 \n",
      "Epoch:  1\n",
      "15/800.0 loss: 0.6799650862812996 \n",
      "Epoch:  1\n",
      "16/800.0 loss: 0.6886548014248118 \n",
      "Epoch:  1\n",
      "17/800.0 loss: 0.7010260588592954 \n",
      "Epoch:  1\n",
      "18/800.0 loss: 0.7013108573461834 \n",
      "Epoch:  1\n",
      "19/800.0 loss: 0.7033329159021378 \n",
      "Epoch:  1\n",
      "20/800.0 loss: 0.7100825196220761 \n",
      "Epoch:  1\n",
      "21/800.0 loss: 0.7068696238777854 \n",
      "Epoch:  1\n",
      "22/800.0 loss: 0.710061112175817 \n",
      "Epoch:  1\n",
      "23/800.0 loss: 0.7063555816809336 \n",
      "Epoch:  1\n",
      "24/800.0 loss: 0.6980149257183075 \n",
      "Epoch:  1\n",
      "25/800.0 loss: 0.6917943874230752 \n",
      "Epoch:  1\n",
      "26/800.0 loss: 0.6957688607551433 \n",
      "Epoch:  1\n",
      "27/800.0 loss: 0.6997475421854428 \n",
      "Epoch:  1\n",
      "28/800.0 loss: 0.6939385276416252 \n",
      "Epoch:  1\n",
      "29/800.0 loss: 0.6978559285402298 \n",
      "Epoch:  1\n",
      "30/800.0 loss: 0.7015606386046256 \n",
      "Epoch:  1\n",
      "31/800.0 loss: 0.7041479228064418 \n",
      "Epoch:  1\n",
      "32/800.0 loss: 0.7050110959645474 \n",
      "Epoch:  1\n",
      "33/800.0 loss: 0.7063924398492364 \n",
      "Epoch:  1\n",
      "34/800.0 loss: 0.7027164995670319 \n",
      "Epoch:  1\n",
      "35/800.0 loss: 0.7001454954346021 \n",
      "Epoch:  1\n",
      "36/800.0 loss: 0.6972821862310976 \n",
      "Epoch:  1\n",
      "37/800.0 loss: 0.6928715039240686 \n",
      "Epoch:  1\n",
      "38/800.0 loss: 0.6963413365376301 \n",
      "Epoch:  1\n",
      "39/800.0 loss: 0.6967678479850292 \n",
      "Epoch:  1\n",
      "40/800.0 loss: 0.6952146037322718 \n",
      "Epoch:  1\n",
      "41/800.0 loss: 0.6943261786585763 \n",
      "Epoch:  1\n",
      "42/800.0 loss: 0.69266589228497 \n",
      "Epoch:  1\n",
      "43/800.0 loss: 0.6932648901234973 \n",
      "Epoch:  1\n",
      "44/800.0 loss: 0.69179381330808 \n",
      "Epoch:  1\n",
      "45/800.0 loss: 0.6895736190287963 \n",
      "Epoch:  1\n",
      "46/800.0 loss: 0.6879974267584212 \n",
      "Epoch:  1\n",
      "47/800.0 loss: 0.6849270481616259 \n",
      "Epoch:  1\n",
      "48/800.0 loss: 0.6818517014688376 \n",
      "Epoch:  1\n",
      "49/800.0 loss: 0.6822944360971451 \n",
      "Epoch:  1\n",
      "50/800.0 loss: 0.682266058290706 \n",
      "Epoch:  1\n",
      "51/800.0 loss: 0.6801350202697974 \n",
      "Epoch:  1\n",
      "52/800.0 loss: 0.6793940938868612 \n",
      "Epoch:  1\n",
      "53/800.0 loss: 0.678145174075056 \n",
      "Epoch:  1\n",
      "54/800.0 loss: 0.6790194538506594 \n",
      "Epoch:  1\n",
      "55/800.0 loss: 0.6782236019415515 \n",
      "Epoch:  1\n",
      "56/800.0 loss: 0.6769847707790241 \n",
      "Epoch:  1\n",
      "57/800.0 loss: 0.6744278263429115 \n",
      "Epoch:  1\n",
      "58/800.0 loss: 0.6734538598585937 \n",
      "Epoch:  1\n",
      "59/800.0 loss: 0.6726871793468793 \n",
      "Epoch:  1\n",
      "60/800.0 loss: 0.6713707403081363 \n",
      "Epoch:  1\n",
      "61/800.0 loss: 0.6699558135963255 \n",
      "Epoch:  1\n",
      "62/800.0 loss: 0.6713243670879848 \n",
      "Epoch:  1\n",
      "63/800.0 loss: 0.6701485835947096 \n",
      "Epoch:  1\n",
      "64/800.0 loss: 0.6679425060749054 \n",
      "Epoch:  1\n",
      "65/800.0 loss: 0.6683297225020148 \n",
      "Epoch:  1\n",
      "66/800.0 loss: 0.6684423511597648 \n",
      "Epoch:  1\n",
      "67/800.0 loss: 0.6677808012155926 \n",
      "Epoch:  1\n",
      "68/800.0 loss: 0.6662458151146986 \n",
      "Epoch:  1\n",
      "69/800.0 loss: 0.6648423488651003 \n",
      "Epoch:  1\n",
      "70/800.0 loss: 0.6645230629914244 \n",
      "Epoch:  1\n",
      "71/800.0 loss: 0.6646591478751765 \n",
      "Epoch:  1\n",
      "72/800.0 loss: 0.6629057889931822 \n",
      "Epoch:  1\n",
      "73/800.0 loss: 0.6618775318603258 \n",
      "Epoch:  1\n",
      "74/800.0 loss: 0.6586504765351613 \n",
      "Epoch:  1\n",
      "75/800.0 loss: 0.6576790625327512 \n",
      "Epoch:  1\n",
      "76/800.0 loss: 0.6549541551571387 \n",
      "Epoch:  1\n",
      "77/800.0 loss: 0.6537451686767431 \n",
      "Epoch:  1\n",
      "78/800.0 loss: 0.65260506619381 \n",
      "Epoch:  1\n",
      "79/800.0 loss: 0.6531336437910795 \n",
      "Epoch:  1\n",
      "80/800.0 loss: 0.6524254624490384 \n",
      "Epoch:  1\n",
      "81/800.0 loss: 0.6510262870933952 \n",
      "Epoch:  1\n",
      "82/800.0 loss: 0.650422453521246 \n",
      "Epoch:  1\n",
      "83/800.0 loss: 0.6493182263913608 \n",
      "Epoch:  1\n",
      "84/800.0 loss: 0.6473350714234745 \n",
      "Epoch:  1\n",
      "85/800.0 loss: 0.6462671548821205 \n",
      "Epoch:  1\n",
      "86/800.0 loss: 0.6460908370456476 \n",
      "Epoch:  1\n",
      "87/800.0 loss: 0.6472798816182397 \n",
      "Epoch:  1\n",
      "88/800.0 loss: 0.6466343201948016 \n",
      "Epoch:  1\n",
      "89/800.0 loss: 0.6439085877603955 \n",
      "Epoch:  1\n",
      "90/800.0 loss: 0.6440441788552882 \n",
      "Epoch:  1\n",
      "91/800.0 loss: 0.6413171032200689 \n",
      "Epoch:  1\n",
      "92/800.0 loss: 0.6397885314879879 \n",
      "Epoch:  1\n",
      "93/800.0 loss: 0.636793408305087 \n",
      "Epoch:  1\n",
      "94/800.0 loss: 0.6373191303328464 \n",
      "Epoch:  1\n",
      "95/800.0 loss: 0.6352494135499 \n",
      "Epoch:  1\n",
      "96/800.0 loss: 0.6369450811258296 \n",
      "Epoch:  1\n",
      "97/800.0 loss: 0.6355321921256124 \n",
      "Epoch:  1\n",
      "98/800.0 loss: 0.6328857739766439 \n",
      "Epoch:  1\n",
      "99/800.0 loss: 0.6317036813497543 \n",
      "Epoch:  1\n",
      "100/800.0 loss: 0.6308895231473564 \n",
      "Epoch:  1\n",
      "101/800.0 loss: 0.6298224920151281 \n",
      "Epoch:  1\n",
      "102/800.0 loss: 0.630223905577243 \n",
      "Epoch:  1\n",
      "103/800.0 loss: 0.6284630579444078 \n",
      "Epoch:  1\n",
      "104/800.0 loss: 0.6266196239562262 \n",
      "Epoch:  1\n",
      "105/800.0 loss: 0.6256743579540612 \n",
      "Epoch:  1\n",
      "106/800.0 loss: 0.6236915624587336 \n",
      "Epoch:  1\n",
      "107/800.0 loss: 0.6221952899186699 \n",
      "Epoch:  1\n",
      "108/800.0 loss: 0.6204321234051241 \n",
      "Epoch:  1\n",
      "109/800.0 loss: 0.6198407435959036 \n",
      "Epoch:  1\n",
      "110/800.0 loss: 0.6188648335031561 \n",
      "Epoch:  1\n",
      "111/800.0 loss: 0.6174241629030023 \n",
      "Epoch:  1\n",
      "112/800.0 loss: 0.6158624784608858 \n",
      "Epoch:  1\n",
      "113/800.0 loss: 0.6151829202447021 \n",
      "Epoch:  1\n",
      "114/800.0 loss: 0.6148177102856014 \n",
      "Epoch:  1\n",
      "115/800.0 loss: 0.615371590287521 \n",
      "Epoch:  1\n",
      "116/800.0 loss: 0.6136267271816221 \n",
      "Epoch:  1\n",
      "117/800.0 loss: 0.6118072589575234 \n",
      "Epoch:  1\n",
      "118/800.0 loss: 0.611435480478431 \n",
      "Epoch:  1\n",
      "119/800.0 loss: 0.6102363934119542 \n",
      "Epoch:  1\n",
      "120/800.0 loss: 0.6093849510200753 \n",
      "Epoch:  1\n",
      "121/800.0 loss: 0.6082311316591794 \n",
      "Epoch:  1\n",
      "122/800.0 loss: 0.605944620157645 \n",
      "Epoch:  1\n",
      "123/800.0 loss: 0.6038623742999569 \n",
      "Epoch:  1\n",
      "124/800.0 loss: 0.6023057756423951 \n",
      "Epoch:  1\n",
      "125/800.0 loss: 0.6005553087544819 \n",
      "Epoch:  1\n",
      "126/800.0 loss: 0.599585840317208 \n",
      "Epoch:  1\n",
      "127/800.0 loss: 0.5977748546283692 \n",
      "Epoch:  1\n",
      "128/800.0 loss: 0.5967375904090645 \n",
      "Epoch:  1\n",
      "129/800.0 loss: 0.5955827424159417 \n",
      "Epoch:  1\n",
      "130/800.0 loss: 0.5942371469417601 \n",
      "Epoch:  1\n",
      "131/800.0 loss: 0.5926559341676307 \n",
      "Epoch:  1\n",
      "132/800.0 loss: 0.5917494949093438 \n",
      "Epoch:  1\n",
      "133/800.0 loss: 0.5899843864921314 \n",
      "Epoch:  1\n",
      "134/800.0 loss: 0.5881121096787629 \n",
      "Epoch:  1\n",
      "135/800.0 loss: 0.586116602534757 \n",
      "Epoch:  1\n",
      "136/800.0 loss: 0.5861211986872401 \n",
      "Epoch:  1\n",
      "137/800.0 loss: 0.5838182084802268 \n",
      "Epoch:  1\n",
      "138/800.0 loss: 0.5819337001378587 \n",
      "Epoch:  1\n",
      "139/800.0 loss: 0.5803492573755128 \n",
      "Epoch:  1\n",
      "140/800.0 loss: 0.5805085654377092 \n",
      "Epoch:  1\n",
      "141/800.0 loss: 0.5791204569625182 \n",
      "Epoch:  1\n",
      "142/800.0 loss: 0.5768720843158401 \n",
      "Epoch:  1\n",
      "143/800.0 loss: 0.5750844017085102 \n",
      "Epoch:  1\n",
      "144/800.0 loss: 0.5732042649696614 \n",
      "Epoch:  1\n",
      "145/800.0 loss: 0.5716803571949266 \n",
      "Epoch:  1\n",
      "146/800.0 loss: 0.5701625817487029 \n",
      "Epoch:  1\n",
      "147/800.0 loss: 0.5679839057092732 \n",
      "Epoch:  1\n",
      "148/800.0 loss: 0.5678394557845673 \n",
      "Epoch:  1\n",
      "149/800.0 loss: 0.5671835387746493 \n",
      "Epoch:  1\n",
      "150/800.0 loss: 0.5655439607552345 \n",
      "Epoch:  1\n",
      "151/800.0 loss: 0.5651777120993325 \n",
      "Epoch:  1\n",
      "152/800.0 loss: 0.5645693516224818 \n",
      "Epoch:  1\n",
      "153/800.0 loss: 0.5627763239400727 \n",
      "Epoch:  1\n",
      "154/800.0 loss: 0.5609273544242305 \n",
      "Epoch:  1\n",
      "155/800.0 loss: 0.5598109659667199 \n",
      "Epoch:  1\n",
      "156/800.0 loss: 0.5587728125084738 \n",
      "Epoch:  1\n",
      "157/800.0 loss: 0.5576234892765178 \n",
      "Epoch:  1\n",
      "158/800.0 loss: 0.5556925511772528 \n",
      "Epoch:  1\n",
      "159/800.0 loss: 0.557566715683788 \n",
      "Epoch:  1\n",
      "160/800.0 loss: 0.5558290164107862 \n",
      "Epoch:  1\n",
      "161/800.0 loss: 0.5542692180584978 \n",
      "Epoch:  1\n",
      "162/800.0 loss: 0.552350775047314 \n",
      "Epoch:  1\n",
      "163/800.0 loss: 0.5505468941679815 \n",
      "Epoch:  1\n",
      "164/800.0 loss: 0.5530620547858152 \n",
      "Epoch:  1\n",
      "165/800.0 loss: 0.5526636027428041 \n",
      "Epoch:  1\n",
      "166/800.0 loss: 0.5508414222273285 \n",
      "Epoch:  1\n",
      "167/800.0 loss: 0.5500817102867932 \n",
      "Epoch:  1\n",
      "168/800.0 loss: 0.5481520993469735 \n",
      "Epoch:  1\n",
      "169/800.0 loss: 0.5461254417019732 \n",
      "Epoch:  1\n",
      "170/800.0 loss: 0.544370902647749 \n",
      "Epoch:  1\n",
      "171/800.0 loss: 0.5448104230643704 \n",
      "Epoch:  1\n",
      "172/800.0 loss: 0.5429672690140719 \n",
      "Epoch:  1\n",
      "173/800.0 loss: 0.5413457235728187 \n",
      "Epoch:  1\n",
      "174/800.0 loss: 0.5395243914638247 \n",
      "Epoch:  1\n",
      "175/800.0 loss: 0.5387455329129641 \n",
      "Epoch:  1\n",
      "176/800.0 loss: 0.5375894590117837 \n",
      "Epoch:  1\n",
      "177/800.0 loss: 0.5370849629633883 \n",
      "Epoch:  1\n",
      "178/800.0 loss: 0.5354587747065048 \n",
      "Epoch:  1\n",
      "179/800.0 loss: 0.5338833808898926 \n",
      "Epoch:  1\n",
      "180/800.0 loss: 0.5333051269884268 \n",
      "Epoch:  1\n",
      "181/800.0 loss: 0.5314404069558605 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "182/800.0 loss: 0.5311396026709041 \n",
      "Epoch:  1\n",
      "183/800.0 loss: 0.5300990309890198 \n",
      "Epoch:  1\n",
      "184/800.0 loss: 0.5284550044182185 \n",
      "Epoch:  1\n",
      "185/800.0 loss: 0.5267097343520452 \n",
      "Epoch:  1\n",
      "186/800.0 loss: 0.5259555829240676 \n",
      "Epoch:  1\n",
      "187/800.0 loss: 0.5253048394113145 \n",
      "Epoch:  1\n",
      "188/800.0 loss: 0.5245268609000262 \n",
      "Epoch:  1\n",
      "189/800.0 loss: 0.523483676894715 \n",
      "Epoch:  1\n",
      "190/800.0 loss: 0.521571415335096 \n",
      "Epoch:  1\n",
      "191/800.0 loss: 0.5210151555171857 \n",
      "Epoch:  1\n",
      "192/800.0 loss: 0.5201916715487297 \n",
      "Epoch:  1\n",
      "193/800.0 loss: 0.5185127943446955 \n",
      "Epoch:  1\n",
      "194/800.0 loss: 0.5172101552669819 \n",
      "Epoch:  1\n",
      "195/800.0 loss: 0.5163006166718445 \n",
      "Epoch:  1\n",
      "196/800.0 loss: 0.5154228608317787 \n",
      "Epoch:  1\n",
      "197/800.0 loss: 0.5150930723457625 \n",
      "Epoch:  1\n",
      "198/800.0 loss: 0.5133589774220433 \n",
      "Epoch:  1\n",
      "199/800.0 loss: 0.5121009455621243 \n",
      "Epoch:  1\n",
      "200/800.0 loss: 0.510505895709517 \n",
      "Epoch:  1\n",
      "201/800.0 loss: 0.5095189656361495 \n",
      "Epoch:  1\n",
      "202/800.0 loss: 0.5086823255557732 \n",
      "Epoch:  1\n",
      "203/800.0 loss: 0.5077499729745528 \n",
      "Epoch:  1\n",
      "204/800.0 loss: 0.506438540612779 \n",
      "Epoch:  1\n",
      "205/800.0 loss: 0.5050984427188207 \n",
      "Epoch:  1\n",
      "206/800.0 loss: 0.5040962742146663 \n",
      "Epoch:  1\n",
      "207/800.0 loss: 0.5024208177167636 \n",
      "Epoch:  1\n",
      "208/800.0 loss: 0.5008264806424602 \n",
      "Epoch:  1\n",
      "209/800.0 loss: 0.49970710284653164 \n",
      "Epoch:  1\n",
      "210/800.0 loss: 0.4981746426408325 \n",
      "Epoch:  1\n",
      "211/800.0 loss: 0.49684573590474307 \n",
      "Epoch:  1\n",
      "212/800.0 loss: 0.49606045024215895 \n",
      "Epoch:  1\n",
      "213/800.0 loss: 0.4944633301869731 \n",
      "Epoch:  1\n",
      "214/800.0 loss: 0.49356726913951166 \n",
      "Epoch:  1\n",
      "215/800.0 loss: 0.49277586952127794 \n",
      "Epoch:  1\n",
      "216/800.0 loss: 0.4919931120174821 \n",
      "Epoch:  1\n",
      "217/800.0 loss: 0.49045672554761993 \n",
      "Epoch:  1\n",
      "218/800.0 loss: 0.48974431397980206 \n",
      "Epoch:  1\n",
      "219/800.0 loss: 0.4881981128318743 \n",
      "Epoch:  1\n",
      "220/800.0 loss: 0.48667786705278165 \n",
      "Epoch:  1\n",
      "221/800.0 loss: 0.48516048806476164 \n",
      "Epoch:  1\n",
      "222/800.0 loss: 0.48430069459126135 \n",
      "Epoch:  1\n",
      "223/800.0 loss: 0.4829104248700397 \n",
      "Epoch:  1\n",
      "224/800.0 loss: 0.48155954248375366 \n",
      "Epoch:  1\n",
      "225/800.0 loss: 0.4806856657419584 \n",
      "Epoch:  1\n",
      "226/800.0 loss: 0.4799402161185437 \n",
      "Epoch:  1\n",
      "227/800.0 loss: 0.47924076759240086 \n",
      "Epoch:  1\n",
      "228/800.0 loss: 0.47778095207360116 \n",
      "Epoch:  1\n",
      "229/800.0 loss: 0.4767923677745073 \n",
      "Epoch:  1\n",
      "230/800.0 loss: 0.47584361095965166 \n",
      "Epoch:  1\n",
      "231/800.0 loss: 0.47588302092305546 \n",
      "Epoch:  1\n",
      "232/800.0 loss: 0.47495348118405484 \n",
      "Epoch:  1\n",
      "233/800.0 loss: 0.4735297485549226 \n",
      "Epoch:  1\n",
      "234/800.0 loss: 0.47228297863868957 \n",
      "Epoch:  1\n",
      "235/800.0 loss: 0.4714030837482315 \n",
      "Epoch:  1\n",
      "236/800.0 loss: 0.4704935411737941 \n",
      "Epoch:  1\n",
      "237/800.0 loss: 0.46922224259176176 \n",
      "Epoch:  1\n",
      "238/800.0 loss: 0.46900491360341157 \n",
      "Epoch:  1\n",
      "239/800.0 loss: 0.46833553922673066 \n",
      "Epoch:  1\n",
      "240/800.0 loss: 0.4672610224900899 \n",
      "Epoch:  1\n",
      "241/800.0 loss: 0.4664353313894311 \n",
      "Epoch:  1\n",
      "242/800.0 loss: 0.4656813247090995 \n",
      "Epoch:  1\n",
      "243/800.0 loss: 0.4644840047618405 \n",
      "Epoch:  1\n",
      "244/800.0 loss: 0.4636265003559541 \n",
      "Epoch:  1\n",
      "245/800.0 loss: 0.4623060058650932 \n",
      "Epoch:  1\n",
      "246/800.0 loss: 0.4614169206093197 \n",
      "Epoch:  1\n",
      "247/800.0 loss: 0.4601874861025041 \n",
      "Epoch:  1\n",
      "248/800.0 loss: 0.45899924558089916 \n",
      "Epoch:  1\n",
      "249/800.0 loss: 0.457888349711895 \n",
      "Epoch:  1\n",
      "250/800.0 loss: 0.4568342765370213 \n",
      "Epoch:  1\n",
      "251/800.0 loss: 0.45556337344977593 \n",
      "Epoch:  1\n",
      "252/800.0 loss: 0.45460955757397437 \n",
      "Epoch:  1\n",
      "253/800.0 loss: 0.4533191777941749 \n",
      "Epoch:  1\n",
      "254/800.0 loss: 0.4520627243261711 \n",
      "Epoch:  1\n",
      "255/800.0 loss: 0.4508613407961093 \n",
      "Epoch:  1\n",
      "256/800.0 loss: 0.4497548221730073 \n",
      "Epoch:  1\n",
      "257/800.0 loss: 0.4485174476407295 \n",
      "Epoch:  1\n",
      "258/800.0 loss: 0.4516393212047783 \n",
      "Epoch:  1\n",
      "259/800.0 loss: 0.4504687827367049 \n",
      "Epoch:  1\n",
      "260/800.0 loss: 0.4493113464436769 \n",
      "Epoch:  1\n",
      "261/800.0 loss: 0.4480575675727757 \n",
      "Epoch:  1\n",
      "262/800.0 loss: 0.44738050513394434 \n",
      "Epoch:  1\n",
      "263/800.0 loss: 0.44607875620325405 \n",
      "Epoch:  1\n",
      "264/800.0 loss: 0.4449051023654218 \n",
      "Epoch:  1\n",
      "265/800.0 loss: 0.44375848249161154 \n",
      "Epoch:  1\n",
      "266/800.0 loss: 0.44268628502829693 \n",
      "Epoch:  1\n",
      "267/800.0 loss: 0.44142141827006837 \n",
      "Epoch:  1\n",
      "268/800.0 loss: 0.44071778799078276 \n",
      "Epoch:  1\n",
      "269/800.0 loss: 0.43948729173452766 \n",
      "Epoch:  1\n",
      "270/800.0 loss: 0.4386729050111507 \n",
      "Epoch:  1\n",
      "271/800.0 loss: 0.4378155652597985 \n",
      "Epoch:  1\n",
      "272/800.0 loss: 0.4372388081618281 \n",
      "Epoch:  1\n",
      "273/800.0 loss: 0.43647242807885156 \n",
      "Epoch:  1\n",
      "274/800.0 loss: 0.43538833079012956 \n",
      "Epoch:  1\n",
      "275/800.0 loss: 0.434313391726734 \n",
      "Epoch:  1\n",
      "276/800.0 loss: 0.43447412305683003 \n",
      "Epoch:  1\n",
      "277/800.0 loss: 0.43398231887024086 \n",
      "Epoch:  1\n",
      "278/800.0 loss: 0.43284435658365167 \n",
      "Epoch:  1\n",
      "279/800.0 loss: 0.43170784416475466 \n",
      "Epoch:  1\n",
      "280/800.0 loss: 0.43056150576185925 \n",
      "Epoch:  1\n",
      "281/800.0 loss: 0.4296956864971641 \n",
      "Epoch:  1\n",
      "282/800.0 loss: 0.42894393658469504 \n",
      "Epoch:  1\n",
      "283/800.0 loss: 0.42821102814984996 \n",
      "Epoch:  1\n",
      "284/800.0 loss: 0.42741159294780934 \n",
      "Epoch:  1\n",
      "285/800.0 loss: 0.42658886454113715 \n",
      "Epoch:  1\n",
      "286/800.0 loss: 0.42572545221250646 \n",
      "Epoch:  1\n",
      "287/800.0 loss: 0.42479699787994224 \n",
      "Epoch:  1\n",
      "288/800.0 loss: 0.42393272130340853 \n",
      "Epoch:  1\n",
      "289/800.0 loss: 0.4228190646346273 \n",
      "Epoch:  1\n",
      "290/800.0 loss: 0.42212724201765256 \n",
      "Epoch:  1\n",
      "291/800.0 loss: 0.4211206133796336 \n",
      "Epoch:  1\n",
      "292/800.0 loss: 0.4201188903975812 \n",
      "Epoch:  1\n",
      "293/800.0 loss: 0.4190596320799419 \n",
      "Epoch:  1\n",
      "294/800.0 loss: 0.4183504732988648 \n",
      "Epoch:  1\n",
      "295/800.0 loss: 0.4173469781070142 \n",
      "Epoch:  1\n",
      "296/800.0 loss: 0.41645928655409253 \n",
      "Epoch:  1\n",
      "297/800.0 loss: 0.4156854067672819 \n",
      "Epoch:  1\n",
      "298/800.0 loss: 0.41492310908925173 \n",
      "Epoch:  1\n",
      "299/800.0 loss: 0.4141308065255483 \n",
      "Epoch:  1\n",
      "300/800.0 loss: 0.4132751069987731 \n",
      "Epoch:  1\n",
      "301/800.0 loss: 0.4126889050895015 \n",
      "Epoch:  1\n",
      "302/800.0 loss: 0.4118721631690614 \n",
      "Epoch:  1\n",
      "303/800.0 loss: 0.4111649540105933 \n",
      "Epoch:  1\n",
      "304/800.0 loss: 0.4101307450747881 \n",
      "Epoch:  1\n",
      "305/800.0 loss: 0.40911905379856334 \n",
      "Epoch:  1\n",
      "306/800.0 loss: 0.4089996880739442 \n",
      "Epoch:  1\n",
      "307/800.0 loss: 0.4084766766951456 \n",
      "Epoch:  1\n",
      "308/800.0 loss: 0.4075459707709192 \n",
      "Epoch:  1\n",
      "309/800.0 loss: 0.40658035691707367 \n",
      "Epoch:  1\n",
      "310/800.0 loss: 0.40574768659961186 \n",
      "Epoch:  1\n",
      "311/800.0 loss: 0.4047696702898695 \n",
      "Epoch:  1\n",
      "312/800.0 loss: 0.4037749121507136 \n",
      "Epoch:  1\n",
      "313/800.0 loss: 0.4032199558606193 \n",
      "Epoch:  1\n",
      "314/800.0 loss: 0.4024244622815223 \n",
      "Epoch:  1\n",
      "315/800.0 loss: 0.40158674740998807 \n",
      "Epoch:  1\n",
      "316/800.0 loss: 0.4023318478502686 \n",
      "Epoch:  1\n",
      "317/800.0 loss: 0.4014430993804767 \n",
      "Epoch:  1\n",
      "318/800.0 loss: 0.40086317153466533 \n",
      "Epoch:  1\n",
      "319/800.0 loss: 0.40008525250013915 \n",
      "Epoch:  1\n",
      "320/800.0 loss: 0.3993924690694824 \n",
      "Epoch:  1\n",
      "321/800.0 loss: 0.3987824725484626 \n",
      "Epoch:  1\n",
      "322/800.0 loss: 0.3978794127379778 \n",
      "Epoch:  1\n",
      "323/800.0 loss: 0.3970812558437939 \n",
      "Epoch:  1\n",
      "324/800.0 loss: 0.3965645005382024 \n",
      "Epoch:  1\n",
      "325/800.0 loss: 0.3956882078771942 \n",
      "Epoch:  1\n",
      "326/800.0 loss: 0.3950312671675959 \n",
      "Epoch:  1\n",
      "327/800.0 loss: 0.39430696449083524 \n",
      "Epoch:  1\n",
      "328/800.0 loss: 0.3934247197532364 \n",
      "Epoch:  1\n",
      "329/800.0 loss: 0.3927630056485985 \n",
      "Epoch:  1\n",
      "330/800.0 loss: 0.39210321346798693 \n",
      "Epoch:  1\n",
      "331/800.0 loss: 0.39130148776324397 \n",
      "Epoch:  1\n",
      "332/800.0 loss: 0.39063018421690027 \n",
      "Epoch:  1\n",
      "333/800.0 loss: 0.3897553666712281 \n",
      "Epoch:  1\n",
      "334/800.0 loss: 0.3892699310139044 \n",
      "Epoch:  1\n",
      "335/800.0 loss: 0.38862667028747855 \n",
      "Epoch:  1\n",
      "336/800.0 loss: 0.3878906206048207 \n",
      "Epoch:  1\n",
      "337/800.0 loss: 0.38719682518899795 \n",
      "Epoch:  1\n",
      "338/800.0 loss: 0.3867511067066924 \n",
      "Epoch:  1\n",
      "339/800.0 loss: 0.38596814465435114 \n",
      "Epoch:  1\n",
      "340/800.0 loss: 0.38512382533980255 \n",
      "Epoch:  1\n",
      "341/800.0 loss: 0.38432370779807107 \n",
      "Epoch:  1\n",
      "342/800.0 loss: 0.38346287647245925 \n",
      "Epoch:  1\n",
      "343/800.0 loss: 0.3826386389257603 \n",
      "Epoch:  1\n",
      "344/800.0 loss: 0.3818677722111992 \n",
      "Epoch:  1\n",
      "345/800.0 loss: 0.3810382130087456 \n",
      "Epoch:  1\n",
      "346/800.0 loss: 0.38020052455945386 \n",
      "Epoch:  1\n",
      "347/800.0 loss: 0.37942187764264385 \n",
      "Epoch:  1\n",
      "348/800.0 loss: 0.378594973111221 \n",
      "Epoch:  1\n",
      "349/800.0 loss: 0.37789721093007494 \n",
      "Epoch:  1\n",
      "350/800.0 loss: 0.37708342336436623 \n",
      "Epoch:  1\n",
      "351/800.0 loss: 0.37622442589649424 \n",
      "Epoch:  1\n",
      "352/800.0 loss: 0.37538615512864787 \n",
      "Epoch:  1\n",
      "353/800.0 loss: 0.37456684898437753 \n",
      "Epoch:  1\n",
      "354/800.0 loss: 0.37381625800905094 \n",
      "Epoch:  1\n",
      "355/800.0 loss: 0.3730037203791101 \n",
      "Epoch:  1\n",
      "356/800.0 loss: 0.372205007435227 \n",
      "Epoch:  1\n",
      "357/800.0 loss: 0.37161441080730057 \n",
      "Epoch:  1\n",
      "358/800.0 loss: 0.37093735280807305 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "359/800.0 loss: 0.37012427219500144 \n",
      "Epoch:  1\n",
      "360/800.0 loss: 0.36945133268255276 \n",
      "Epoch:  1\n",
      "361/800.0 loss: 0.36868361062601784 \n",
      "Epoch:  1\n",
      "362/800.0 loss: 0.3678928053740299 \n",
      "Epoch:  1\n",
      "363/800.0 loss: 0.367072213359259 \n",
      "Epoch:  1\n",
      "364/800.0 loss: 0.36627642073451655 \n",
      "Epoch:  1\n",
      "365/800.0 loss: 0.36561136816938716 \n",
      "Epoch:  1\n",
      "366/800.0 loss: 0.36489207405160495 \n",
      "Epoch:  1\n",
      "367/800.0 loss: 0.3642006463209248 \n",
      "Epoch:  1\n",
      "368/800.0 loss: 0.3634364438775755 \n",
      "Epoch:  1\n",
      "369/800.0 loss: 0.3626897105695428 \n",
      "Epoch:  1\n",
      "370/800.0 loss: 0.3620248606825132 \n",
      "Epoch:  1\n",
      "371/800.0 loss: 0.36133277354141075 \n",
      "Epoch:  1\n",
      "372/800.0 loss: 0.3605846480613739 \n",
      "Epoch:  1\n",
      "373/800.0 loss: 0.3598412141243723 \n",
      "Epoch:  1\n",
      "374/800.0 loss: 0.35906537433465324 \n",
      "Epoch:  1\n",
      "375/800.0 loss: 0.3583896152001429 \n",
      "Epoch:  1\n",
      "376/800.0 loss: 0.35770695020175425 \n",
      "Epoch:  1\n",
      "377/800.0 loss: 0.35696319910505464 \n",
      "Epoch:  1\n",
      "378/800.0 loss: 0.35620460181563385 \n",
      "Epoch:  1\n",
      "379/800.0 loss: 0.3555578691982909 \n",
      "Epoch:  1\n",
      "380/800.0 loss: 0.35491609248745787 \n",
      "Epoch:  1\n",
      "381/800.0 loss: 0.3542038740795008 \n",
      "Epoch:  1\n",
      "382/800.0 loss: 0.35363870588746454 \n",
      "Epoch:  1\n",
      "383/800.0 loss: 0.35290749155683443 \n",
      "Epoch:  1\n",
      "384/800.0 loss: 0.3521379567392461 \n",
      "Epoch:  1\n",
      "385/800.0 loss: 0.3515010612770683 \n",
      "Epoch:  1\n",
      "386/800.0 loss: 0.3508598011477973 \n",
      "Epoch:  1\n",
      "387/800.0 loss: 0.35016129741963653 \n",
      "Epoch:  1\n",
      "388/800.0 loss: 0.349509116846369 \n",
      "Epoch:  1\n",
      "389/800.0 loss: 0.3488753868983342 \n",
      "Epoch:  1\n",
      "390/800.0 loss: 0.3482941636039168 \n",
      "Epoch:  1\n",
      "391/800.0 loss: 0.3476512028588628 \n",
      "Epoch:  1\n",
      "392/800.0 loss: 0.34693722246788233 \n",
      "Epoch:  1\n",
      "393/800.0 loss: 0.3463564046899679 \n",
      "Epoch:  1\n",
      "394/800.0 loss: 0.34576064825435227 \n",
      "Epoch:  1\n",
      "395/800.0 loss: 0.3453610247621934 \n",
      "Epoch:  1\n",
      "396/800.0 loss: 0.34474447255095547 \n",
      "Epoch:  1\n",
      "397/800.0 loss: 0.3441178170617801 \n",
      "Epoch:  1\n",
      "398/800.0 loss: 0.3434337358688352 \n",
      "Epoch:  1\n",
      "399/800.0 loss: 0.3427884056977928 \n",
      "Epoch:  1\n",
      "400/800.0 loss: 0.34217005865903866 \n",
      "Epoch:  1\n",
      "401/800.0 loss: 0.3414483858893315 \n",
      "Epoch:  1\n",
      "402/800.0 loss: 0.34080804865021563 \n",
      "Epoch:  1\n",
      "403/800.0 loss: 0.3401651043389546 \n",
      "Epoch:  1\n",
      "404/800.0 loss: 0.33950258965293567 \n",
      "Epoch:  1\n",
      "405/800.0 loss: 0.3388695127326133 \n",
      "Epoch:  1\n",
      "406/800.0 loss: 0.33822899351139035 \n",
      "Epoch:  1\n",
      "407/800.0 loss: 0.3376499468223283 \n",
      "Epoch:  1\n",
      "408/800.0 loss: 0.33697742900485517 \n",
      "Epoch:  1\n",
      "409/800.0 loss: 0.33631880802170533 \n",
      "Epoch:  1\n",
      "410/800.0 loss: 0.33564835640889595 \n",
      "Epoch:  1\n",
      "411/800.0 loss: 0.3350372919089441 \n",
      "Epoch:  1\n",
      "412/800.0 loss: 0.3343968326744531 \n",
      "Epoch:  1\n",
      "413/800.0 loss: 0.3337724489668285 \n",
      "Epoch:  1\n",
      "414/800.0 loss: 0.33315399182309585 \n",
      "Epoch:  1\n",
      "415/800.0 loss: 0.3326438698075855 \n",
      "Epoch:  1\n",
      "416/800.0 loss: 0.33207150501348703 \n",
      "Epoch:  1\n",
      "417/800.0 loss: 0.33145108541839147 \n",
      "Epoch:  1\n",
      "418/800.0 loss: 0.3308530527129236 \n",
      "Epoch:  1\n",
      "419/800.0 loss: 0.3302374265644522 \n",
      "Epoch:  1\n",
      "420/800.0 loss: 0.32962659603944866 \n",
      "Epoch:  1\n",
      "421/800.0 loss: 0.32904636601242127 \n",
      "Epoch:  1\n",
      "422/800.0 loss: 0.3284257600792897 \n",
      "Epoch:  1\n",
      "423/800.0 loss: 0.32786049204439205 \n",
      "Epoch:  1\n",
      "424/800.0 loss: 0.3273013687922674 \n",
      "Epoch:  1\n",
      "425/800.0 loss: 0.32683519939476613 \n",
      "Epoch:  1\n",
      "426/800.0 loss: 0.3262547879486369 \n",
      "Epoch:  1\n",
      "427/800.0 loss: 0.3256070590621539 \n",
      "Epoch:  1\n",
      "428/800.0 loss: 0.3250401720039911 \n",
      "Epoch:  1\n",
      "429/800.0 loss: 0.32447766007552314 \n",
      "Epoch:  1\n",
      "430/800.0 loss: 0.3239139933441604 \n",
      "Epoch:  1\n",
      "431/800.0 loss: 0.3233688180938501 \n",
      "Epoch:  1\n",
      "432/800.0 loss: 0.3227820469122423 \n",
      "Epoch:  1\n",
      "433/800.0 loss: 0.3222111613674235 \n",
      "Epoch:  1\n",
      "434/800.0 loss: 0.32161536039463406 \n",
      "Epoch:  1\n",
      "435/800.0 loss: 0.32113618763385837 \n",
      "Epoch:  1\n",
      "436/800.0 loss: 0.3205125463073668 \n",
      "Epoch:  1\n",
      "437/800.0 loss: 0.3199194970808617 \n",
      "Epoch:  1\n",
      "438/800.0 loss: 0.31937298081934046 \n",
      "Epoch:  1\n",
      "439/800.0 loss: 0.31882989939979534 \n",
      "Epoch:  1\n",
      "440/800.0 loss: 0.3182384910615258 \n",
      "Epoch:  1\n",
      "441/800.0 loss: 0.3177214289564488 \n",
      "Epoch:  1\n",
      "442/800.0 loss: 0.3171448460433203 \n",
      "Epoch:  1\n",
      "443/800.0 loss: 0.31663857683170216 \n",
      "Epoch:  1\n",
      "444/800.0 loss: 0.3160819760450486 \n",
      "Epoch:  1\n",
      "445/800.0 loss: 0.3155045391649275 \n",
      "Epoch:  1\n",
      "446/800.0 loss: 0.31495406336132314 \n",
      "Epoch:  1\n",
      "447/800.0 loss: 0.3144160218653269 \n",
      "Epoch:  1\n",
      "448/800.0 loss: 0.31386735447099584 \n",
      "Epoch:  1\n",
      "449/800.0 loss: 0.3132839046915372 \n",
      "Epoch:  1\n",
      "450/800.0 loss: 0.3126991968560642 \n",
      "Epoch:  1\n",
      "451/800.0 loss: 0.3121246856480705 \n",
      "Epoch:  1\n",
      "452/800.0 loss: 0.31155069854941064 \n",
      "Epoch:  1\n",
      "453/800.0 loss: 0.31097819962198253 \n",
      "Epoch:  1\n",
      "454/800.0 loss: 0.3104695164649696 \n",
      "Epoch:  1\n",
      "455/800.0 loss: 0.3099112292147127 \n",
      "Epoch:  1\n",
      "456/800.0 loss: 0.30935782755784186 \n",
      "Epoch:  1\n",
      "457/800.0 loss: 0.308812427899788 \n",
      "Epoch:  1\n",
      "458/800.0 loss: 0.30823096690581775 \n",
      "Epoch:  1\n",
      "459/800.0 loss: 0.30769709302517384 \n",
      "Epoch:  1\n",
      "460/800.0 loss: 0.3071843296564194 \n",
      "Epoch:  1\n",
      "461/800.0 loss: 0.3121428934865313 \n",
      "Epoch:  1\n",
      "462/800.0 loss: 0.3116333476362295 \n",
      "Epoch:  1\n",
      "463/800.0 loss: 0.31111320169995826 \n",
      "Epoch:  1\n",
      "464/800.0 loss: 0.3105612767880322 \n",
      "Epoch:  1\n",
      "465/800.0 loss: 0.3100077065643195 \n",
      "Epoch:  1\n",
      "466/800.0 loss: 0.3094608517048573 \n",
      "Epoch:  1\n",
      "467/800.0 loss: 0.30895753831276274 \n",
      "Epoch:  1\n",
      "468/800.0 loss: 0.30842404697400166 \n",
      "Epoch:  1\n",
      "469/800.0 loss: 0.3078646117781705 \n",
      "Epoch:  1\n",
      "470/800.0 loss: 0.307316001931763 \n",
      "Epoch:  1\n",
      "471/800.0 loss: 0.30675660362625023 \n",
      "Epoch:  1\n",
      "472/800.0 loss: 0.306201980856836 \n",
      "Epoch:  1\n",
      "473/800.0 loss: 0.305725884981422 \n",
      "Epoch:  1\n",
      "474/800.0 loss: 0.30518392230335034 \n",
      "Epoch:  1\n",
      "475/800.0 loss: 0.3047352185582413 \n",
      "Epoch:  1\n",
      "476/800.0 loss: 0.30420887234388405 \n",
      "Epoch:  1\n",
      "477/800.0 loss: 0.3037764028057643 \n",
      "Epoch:  1\n",
      "478/800.0 loss: 0.3032602333932306 \n",
      "Epoch:  1\n",
      "479/800.0 loss: 0.3027951986140882 \n",
      "Epoch:  1\n",
      "480/800.0 loss: 0.30232893276704076 \n",
      "Epoch:  1\n",
      "481/800.0 loss: 0.30181620317932484 \n",
      "Epoch:  1\n",
      "482/800.0 loss: 0.3013352886860415 \n",
      "Epoch:  1\n",
      "483/800.0 loss: 0.3008148623861311 \n",
      "Epoch:  1\n",
      "484/800.0 loss: 0.3004036509038247 \n",
      "Epoch:  1\n",
      "485/800.0 loss: 0.29989035922742674 \n",
      "Epoch:  1\n",
      "486/800.0 loss: 0.2993657312660002 \n",
      "Epoch:  1\n",
      "487/800.0 loss: 0.2988550697270109 \n",
      "Epoch:  1\n",
      "488/800.0 loss: 0.29857742552529326 \n",
      "Epoch:  1\n",
      "489/800.0 loss: 0.2982027747573293 \n",
      "Epoch:  1\n",
      "490/800.0 loss: 0.29778728880701383 \n",
      "Epoch:  1\n",
      "491/800.0 loss: 0.2972849429077734 \n",
      "Epoch:  1\n",
      "492/800.0 loss: 0.29688757274141175 \n",
      "Epoch:  1\n",
      "493/800.0 loss: 0.2963865289336395 \n",
      "Epoch:  1\n",
      "494/800.0 loss: 0.29586933961119316 \n",
      "Epoch:  1\n",
      "495/800.0 loss: 0.29546541857322856 \n",
      "Epoch:  1\n",
      "496/800.0 loss: 0.29497310506476004 \n",
      "Epoch:  1\n",
      "497/800.0 loss: 0.29448222077216013 \n",
      "Epoch:  1\n",
      "498/800.0 loss: 0.29400539735634484 \n",
      "Epoch:  1\n",
      "499/800.0 loss: 0.2935607428997755 \n",
      "Epoch:  1\n",
      "500/800.0 loss: 0.29309167849000345 \n",
      "Epoch:  1\n",
      "501/800.0 loss: 0.2925920755160876 \n",
      "Epoch:  1\n",
      "502/800.0 loss: 0.2921456089298839 \n",
      "Epoch:  1\n",
      "503/800.0 loss: 0.29168304956207675 \n",
      "Epoch:  1\n",
      "504/800.0 loss: 0.29119048790618923 \n",
      "Epoch:  1\n",
      "505/800.0 loss: 0.2906990471137724 \n",
      "Epoch:  1\n",
      "506/800.0 loss: 0.29025220833498344 \n",
      "Epoch:  1\n",
      "507/800.0 loss: 0.28980791370025066 \n",
      "Epoch:  1\n",
      "508/800.0 loss: 0.2893590703533417 \n",
      "Epoch:  1\n",
      "509/800.0 loss: 0.2889272100566065 \n",
      "Epoch:  1\n",
      "510/800.0 loss: 0.2884428760441432 \n",
      "Epoch:  1\n",
      "511/800.0 loss: 0.2879709802582511 \n",
      "Epoch:  1\n",
      "512/800.0 loss: 0.2874904822913992 \n",
      "Epoch:  1\n",
      "513/800.0 loss: 0.2902794492644791 \n",
      "Epoch:  1\n",
      "514/800.0 loss: 0.289812849331828 \n",
      "Epoch:  1\n",
      "515/800.0 loss: 0.2893285692286815 \n",
      "Epoch:  1\n",
      "516/800.0 loss: 0.2888796617289576 \n",
      "Epoch:  1\n",
      "517/800.0 loss: 0.28842531870978677 \n",
      "Epoch:  1\n",
      "518/800.0 loss: 0.28801789545312323 \n",
      "Epoch:  1\n",
      "519/800.0 loss: 0.28759533148258926 \n",
      "Epoch:  1\n",
      "520/800.0 loss: 0.2871326323591473 \n",
      "Epoch:  1\n",
      "521/800.0 loss: 0.2869511187519036 \n",
      "Epoch:  1\n",
      "522/800.0 loss: 0.28673269472175755 \n",
      "Epoch:  1\n",
      "523/800.0 loss: 0.2867110877134304 \n",
      "Epoch:  1\n",
      "524/800.0 loss: 0.28897142392538844 \n",
      "Epoch:  1\n",
      "525/800.0 loss: 0.2885104704917384 \n",
      "Epoch:  1\n",
      "526/800.0 loss: 0.2881181998570685 \n",
      "Epoch:  1\n",
      "527/800.0 loss: 0.2876626073848456 \n",
      "Epoch:  1\n",
      "528/800.0 loss: 0.28720856155841024 \n",
      "Epoch:  1\n",
      "529/800.0 loss: 0.28681580180407695 \n",
      "Epoch:  1\n",
      "530/800.0 loss: 0.2863574492097911 \n",
      "Epoch:  1\n",
      "531/800.0 loss: 0.28595849324746014 \n",
      "Epoch:  1\n",
      "532/800.0 loss: 0.2854887967890803 \n",
      "Epoch:  1\n",
      "533/800.0 loss: 0.28507909039907464 \n",
      "Epoch:  1\n",
      "534/800.0 loss: 0.2847701481082172 \n",
      "Epoch:  1\n",
      "535/800.0 loss: 0.28434725426165247 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "536/800.0 loss: 0.2839167351163299 \n",
      "Epoch:  1\n",
      "537/800.0 loss: 0.28353934203271086 \n",
      "Epoch:  1\n",
      "538/800.0 loss: 0.28310098727829824 \n",
      "Epoch:  1\n",
      "539/800.0 loss: 0.2826654157221869 \n",
      "Epoch:  1\n",
      "540/800.0 loss: 0.28223857824826637 \n",
      "Epoch:  1\n",
      "541/800.0 loss: 0.2818134607080494 \n",
      "Epoch:  1\n",
      "542/800.0 loss: 0.281420288953937 \n",
      "Epoch:  1\n",
      "543/800.0 loss: 0.2810260648770696 \n",
      "Epoch:  1\n",
      "544/800.0 loss: 0.280630404720886 \n",
      "Epoch:  1\n",
      "545/800.0 loss: 0.28021624732776224 \n",
      "Epoch:  1\n",
      "546/800.0 loss: 0.2798180010108451 \n",
      "Epoch:  1\n",
      "547/800.0 loss: 0.279422997531012 \n",
      "Epoch:  1\n",
      "548/800.0 loss: 0.27907088792497775 \n",
      "Epoch:  1\n",
      "549/800.0 loss: 0.27866587459363723 \n",
      "Epoch:  1\n",
      "550/800.0 loss: 0.27831451613084157 \n",
      "Epoch:  1\n",
      "551/800.0 loss: 0.2779061832006319 \n",
      "Epoch:  1\n",
      "552/800.0 loss: 0.2775710156056053 \n",
      "Epoch:  1\n",
      "553/800.0 loss: 0.2771576190105091 \n",
      "Epoch:  1\n",
      "554/800.0 loss: 0.2767575132551494 \n",
      "Epoch:  1\n",
      "555/800.0 loss: 0.2763274118987669 \n",
      "Epoch:  1\n",
      "556/800.0 loss: 0.27592311439704637 \n",
      "Epoch:  1\n",
      "557/800.0 loss: 0.2755234407988714 \n",
      "Epoch:  1\n",
      "558/800.0 loss: 0.2750879436038261 \n",
      "Epoch:  1\n",
      "559/800.0 loss: 0.27471254557105046 \n",
      "Epoch:  1\n",
      "560/800.0 loss: 0.2742928463480902 \n",
      "Epoch:  1\n",
      "561/800.0 loss: 0.2739163407545497 \n",
      "Epoch:  1\n",
      "562/800.0 loss: 0.27349724052854374 \n",
      "Epoch:  1\n",
      "563/800.0 loss: 0.27309060997679724 \n",
      "Epoch:  1\n",
      "564/800.0 loss: 0.27268252395159376 \n",
      "Epoch:  1\n",
      "565/800.0 loss: 0.2723384890244622 \n",
      "Epoch:  1\n",
      "566/800.0 loss: 0.2719239923491995 \n",
      "Epoch:  1\n",
      "567/800.0 loss: 0.2715108789188761 \n",
      "Epoch:  1\n",
      "568/800.0 loss: 0.2710955946563836 \n",
      "Epoch:  1\n",
      "569/800.0 loss: 0.2707088718223467 \n",
      "Epoch:  1\n",
      "570/800.0 loss: 0.27037364858383683 \n",
      "Epoch:  1\n",
      "571/800.0 loss: 0.2699878334712524 \n",
      "Epoch:  1\n",
      "572/800.0 loss: 0.26962264408121767 \n",
      "Epoch:  1\n",
      "573/800.0 loss: 0.26923323801689447 \n",
      "Epoch:  1\n",
      "574/800.0 loss: 0.2688402244513449 \n",
      "Epoch:  1\n",
      "575/800.0 loss: 0.2684552622247591 \n",
      "Epoch:  1\n",
      "576/800.0 loss: 0.2681007832998936 \n",
      "Epoch:  1\n",
      "577/800.0 loss: 0.2677541295820557 \n",
      "Epoch:  1\n",
      "578/800.0 loss: 0.26737499226766137 \n",
      "Epoch:  1\n",
      "579/800.0 loss: 0.2669649056319533 \n",
      "Epoch:  1\n",
      "580/800.0 loss: 0.26657000278313475 \n",
      "Epoch:  1\n",
      "581/800.0 loss: 0.2662050008223331 \n",
      "Epoch:  1\n",
      "582/800.0 loss: 0.26579651902677365 \n",
      "Epoch:  1\n",
      "583/800.0 loss: 0.26540197397276044 \n",
      "Epoch:  1\n",
      "584/800.0 loss: 0.26504596716636775 \n",
      "Epoch:  1\n",
      "585/800.0 loss: 0.2647066606501111 \n",
      "Epoch:  1\n",
      "586/800.0 loss: 0.2643518541833256 \n",
      "Epoch:  1\n",
      "587/800.0 loss: 0.264028694635878 \n",
      "Epoch:  1\n",
      "588/800.0 loss: 0.26366375335800524 \n",
      "Epoch:  1\n",
      "589/800.0 loss: 0.26328275121306466 \n",
      "Epoch:  1\n",
      "590/800.0 loss: 0.2628962728848867 \n",
      "Epoch:  1\n",
      "591/800.0 loss: 0.2625216769614584 \n",
      "Epoch:  1\n",
      "592/800.0 loss: 0.26213406511028037 \n",
      "Epoch:  1\n",
      "593/800.0 loss: 0.2617679141455578 \n",
      "Epoch:  1\n",
      "594/800.0 loss: 0.26138015169055523 \n",
      "Epoch:  1\n",
      "595/800.0 loss: 0.2610123162314096 \n",
      "Epoch:  1\n",
      "596/800.0 loss: 0.26064516672586874 \n",
      "Epoch:  1\n",
      "597/800.0 loss: 0.26028973892032103 \n",
      "Epoch:  1\n",
      "598/800.0 loss: 0.25996296356536314 \n",
      "Epoch:  1\n",
      "599/800.0 loss: 0.2595766010383765 \n",
      "Epoch:  1\n",
      "600/800.0 loss: 0.25921402011878675 \n",
      "Epoch:  1\n",
      "601/800.0 loss: 0.25889891439869356 \n",
      "Epoch:  1\n",
      "602/800.0 loss: 0.2585129632983684 \n",
      "Epoch:  1\n",
      "603/800.0 loss: 0.2581705466400067 \n",
      "Epoch:  1\n",
      "604/800.0 loss: 0.2577978186709575 \n",
      "Epoch:  1\n",
      "605/800.0 loss: 0.25741954961416647 \n",
      "Epoch:  1\n",
      "606/800.0 loss: 0.257034228031433 \n",
      "Epoch:  1\n",
      "607/800.0 loss: 0.25668133373102664 \n",
      "Epoch:  1\n",
      "608/800.0 loss: 0.2563055994047222 \n",
      "Epoch:  1\n",
      "609/800.0 loss: 0.2559681941678778 \n",
      "Epoch:  1\n",
      "610/800.0 loss: 0.2556067031858498 \n",
      "Epoch:  1\n",
      "611/800.0 loss: 0.2552374166229533 \n",
      "Epoch:  1\n",
      "612/800.0 loss: 0.25490590287934606 \n",
      "Epoch:  1\n",
      "613/800.0 loss: 0.2545401681311647 \n",
      "Epoch:  1\n",
      "614/800.0 loss: 0.2541897678036031 \n",
      "Epoch:  1\n",
      "615/800.0 loss: 0.2538257743528282 \n",
      "Epoch:  1\n",
      "616/800.0 loss: 0.25345820165880784 \n",
      "Epoch:  1\n",
      "617/800.0 loss: 0.2531106796109609 \n",
      "Epoch:  1\n",
      "618/800.0 loss: 0.2527481707923343 \n",
      "Epoch:  1\n",
      "619/800.0 loss: 0.2524132891287727 \n",
      "Epoch:  1\n",
      "620/800.0 loss: 0.25205983559866457 \n",
      "Epoch:  1\n",
      "621/800.0 loss: 0.2517366090975103 \n",
      "Epoch:  1\n",
      "622/800.0 loss: 0.251385569135938 \n",
      "Epoch:  1\n",
      "623/800.0 loss: 0.2510303448802099 \n",
      "Epoch:  1\n",
      "624/800.0 loss: 0.25069348801374436 \n",
      "Epoch:  1\n",
      "625/800.0 loss: 0.25036305016555344 \n",
      "Epoch:  1\n",
      "626/800.0 loss: 0.25003335847143543 \n",
      "Epoch:  1\n",
      "627/800.0 loss: 0.24968990618303702 \n",
      "Epoch:  1\n",
      "628/800.0 loss: 0.2493464442026255 \n",
      "Epoch:  1\n",
      "629/800.0 loss: 0.24899797113168806 \n",
      "Epoch:  1\n",
      "630/800.0 loss: 0.24867188104571517 \n",
      "Epoch:  1\n",
      "631/800.0 loss: 0.24835112346100469 \n",
      "Epoch:  1\n",
      "632/800.0 loss: 0.24800694567308795 \n",
      "Epoch:  1\n",
      "633/800.0 loss: 0.24771100202757476 \n",
      "Epoch:  1\n",
      "634/800.0 loss: 0.24737101156176544 \n",
      "Epoch:  1\n",
      "635/800.0 loss: 0.24703831375582413 \n",
      "Epoch:  1\n",
      "636/800.0 loss: 0.24670154401912606 \n",
      "Epoch:  1\n",
      "637/800.0 loss: 0.24636183688154323 \n",
      "Epoch:  1\n",
      "638/800.0 loss: 0.24605908536050522 \n",
      "Epoch:  1\n",
      "639/800.0 loss: 0.24571585493104067 \n",
      "Epoch:  1\n",
      "640/800.0 loss: 0.2453700983998556 \n",
      "Epoch:  1\n",
      "641/800.0 loss: 0.24505838766879753 \n",
      "Epoch:  1\n",
      "642/800.0 loss: 0.2447581547623845 \n",
      "Epoch:  1\n",
      "643/800.0 loss: 0.24442460119539167 \n",
      "Epoch:  1\n",
      "644/800.0 loss: 0.2440867493463348 \n",
      "Epoch:  1\n",
      "645/800.0 loss: 0.24375326067624448 \n",
      "Epoch:  1\n",
      "646/800.0 loss: 0.24342723390003998 \n",
      "Epoch:  1\n",
      "647/800.0 loss: 0.24311169430061622 \n",
      "Epoch:  1\n",
      "648/800.0 loss: 0.2428276168386264 \n",
      "Epoch:  1\n",
      "649/800.0 loss: 0.24249912303800766 \n",
      "Epoch:  1\n",
      "650/800.0 loss: 0.24217410455708221 \n",
      "Epoch:  1\n",
      "651/800.0 loss: 0.24190049426790122 \n",
      "Epoch:  1\n",
      "652/800.0 loss: 0.24160679071787944 \n",
      "Epoch:  1\n",
      "653/800.0 loss: 0.24127804990060378 \n",
      "Epoch:  1\n",
      "654/800.0 loss: 0.24096606554363975 \n",
      "Epoch:  1\n",
      "655/800.0 loss: 0.2406753117712669 \n",
      "Epoch:  1\n",
      "656/800.0 loss: 0.2403509503023492 \n",
      "Epoch:  1\n",
      "657/800.0 loss: 0.24004319262914472 \n",
      "Epoch:  1\n",
      "658/800.0 loss: 0.23974450066832653 \n",
      "Epoch:  1\n",
      "659/800.0 loss: 0.23942477436679782 \n",
      "Epoch:  1\n",
      "660/800.0 loss: 0.2391011900833488 \n",
      "Epoch:  1\n",
      "661/800.0 loss: 0.23880126866655496 \n",
      "Epoch:  1\n",
      "662/800.0 loss: 0.23847610719479498 \n",
      "Epoch:  1\n",
      "663/800.0 loss: 0.23815635411371758 \n",
      "Epoch:  1\n",
      "664/800.0 loss: 0.2378738012949103 \n",
      "Epoch:  1\n",
      "665/800.0 loss: 0.2375606808968045 \n",
      "Epoch:  1\n",
      "666/800.0 loss: 0.23725935614024146 \n",
      "Epoch:  1\n",
      "667/800.0 loss: 0.2369457733856436 \n",
      "Epoch:  1\n",
      "668/800.0 loss: 0.2366298768023427 \n",
      "Epoch:  1\n",
      "669/800.0 loss: 0.2363220034361775 \n",
      "Epoch:  1\n",
      "670/800.0 loss: 0.23601339813793737 \n",
      "Epoch:  1\n",
      "671/800.0 loss: 0.23569929862527975 \n",
      "Epoch:  1\n",
      "672/800.0 loss: 0.23541504221609938 \n",
      "Epoch:  1\n",
      "673/800.0 loss: 0.23510568388337022 \n",
      "Epoch:  1\n",
      "674/800.0 loss: 0.23479254487212056 \n",
      "Epoch:  1\n",
      "675/800.0 loss: 0.2344979647155741 \n",
      "Epoch:  1\n",
      "676/800.0 loss: 0.2342010772095077 \n",
      "Epoch:  1\n",
      "677/800.0 loss: 0.2338962740317031 \n",
      "Epoch:  1\n",
      "678/800.0 loss: 0.23358967157961344 \n",
      "Epoch:  1\n",
      "679/800.0 loss: 0.23328170461361022 \n",
      "Epoch:  1\n",
      "680/800.0 loss: 0.23299211524044014 \n",
      "Epoch:  1\n",
      "681/800.0 loss: 0.23270270068886692 \n",
      "Epoch:  1\n",
      "682/800.0 loss: 0.23241020192011733 \n",
      "Epoch:  1\n",
      "683/800.0 loss: 0.23212805657228183 \n",
      "Epoch:  1\n",
      "684/800.0 loss: 0.23182749877935344 \n",
      "Epoch:  1\n",
      "685/800.0 loss: 0.23151966669588586 \n",
      "Epoch:  1\n",
      "686/800.0 loss: 0.23123895375022807 \n",
      "Epoch:  1\n",
      "687/800.0 loss: 0.23095061010843532 \n",
      "Epoch:  1\n",
      "688/800.0 loss: 0.23067271625906965 \n",
      "Epoch:  1\n",
      "689/800.0 loss: 0.23036541756566453 \n",
      "Epoch:  1\n",
      "690/800.0 loss: 0.23008623334556258 \n",
      "Epoch:  1\n",
      "691/800.0 loss: 0.22978578031472216 \n",
      "Epoch:  1\n",
      "692/800.0 loss: 0.2294851608741034 \n",
      "Epoch:  1\n",
      "693/800.0 loss: 0.22918848215631862 \n",
      "Epoch:  1\n",
      "694/800.0 loss: 0.22890752441913104 \n",
      "Epoch:  1\n",
      "695/800.0 loss: 0.22861504063930835 \n",
      "Epoch:  1\n",
      "696/800.0 loss: 0.22835668985607635 \n",
      "Epoch:  1\n",
      "697/800.0 loss: 0.22806844965722892 \n",
      "Epoch:  1\n",
      "698/800.0 loss: 0.22778175082404128 \n",
      "Epoch:  1\n",
      "699/800.0 loss: 0.2275046904518136 \n",
      "Epoch:  1\n",
      "700/800.0 loss: 0.22720525344666334 \n",
      "Epoch:  1\n",
      "701/800.0 loss: 0.22692746736979552 \n",
      "Epoch:  1\n",
      "702/800.0 loss: 0.22665658531177943 \n",
      "Epoch:  1\n",
      "703/800.0 loss: 0.22640174757477574 \n",
      "Epoch:  1\n",
      "704/800.0 loss: 0.2261115666769498 \n",
      "Epoch:  1\n",
      "705/800.0 loss: 0.22582476366332419 \n",
      "Epoch:  1\n",
      "706/800.0 loss: 0.22554701233685776 \n",
      "Epoch:  1\n",
      "707/800.0 loss: 0.22525717084964283 \n",
      "Epoch:  1\n",
      "708/800.0 loss: 0.22497364621982505 \n",
      "Epoch:  1\n",
      "709/800.0 loss: 0.22469599985215866 \n",
      "Epoch:  1\n",
      "710/800.0 loss: 0.2244094713825507 \n",
      "Epoch:  1\n",
      "711/800.0 loss: 0.2242072304538085 \n",
      "Epoch:  1\n",
      "712/800.0 loss: 0.2239294483066441 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "713/800.0 loss: 0.22365332990415207 \n",
      "Epoch:  1\n",
      "714/800.0 loss: 0.22338645973770352 \n",
      "Epoch:  1\n",
      "715/800.0 loss: 0.22314250850841902 \n",
      "Epoch:  1\n",
      "716/800.0 loss: 0.22285803377555588 \n",
      "Epoch:  1\n",
      "717/800.0 loss: 0.22258887244103098 \n",
      "Epoch:  1\n",
      "718/800.0 loss: 0.22231962392948598 \n",
      "Epoch:  1\n",
      "719/800.0 loss: 0.22204648250869166 \n",
      "Epoch:  1\n",
      "720/800.0 loss: 0.2217647583248422 \n",
      "Epoch:  1\n",
      "721/800.0 loss: 0.22150457197145 \n",
      "Epoch:  1\n",
      "722/800.0 loss: 0.2212341616849929 \n",
      "Epoch:  1\n",
      "723/800.0 loss: 0.22096639398732404 \n",
      "Epoch:  1\n",
      "724/800.0 loss: 0.22070505845135657 \n",
      "Epoch:  1\n",
      "725/800.0 loss: 0.22043047024876125 \n",
      "Epoch:  1\n",
      "726/800.0 loss: 0.22016046559271574 \n",
      "Epoch:  1\n",
      "727/800.0 loss: 0.21988920802199333 \n",
      "Epoch:  1\n",
      "728/800.0 loss: 0.21963540454163882 \n",
      "Epoch:  1\n",
      "729/800.0 loss: 0.21938590278517303 \n",
      "Epoch:  1\n",
      "730/800.0 loss: 0.2191131875484611 \n",
      "Epoch:  1\n",
      "731/800.0 loss: 0.21884980028026987 \n",
      "Epoch:  1\n",
      "732/800.0 loss: 0.2185807795597985 \n",
      "Epoch:  1\n",
      "733/800.0 loss: 0.2183314608105481 \n",
      "Epoch:  1\n",
      "734/800.0 loss: 0.2180752775296062 \n",
      "Epoch:  1\n",
      "735/800.0 loss: 0.21783321784586523 \n",
      "Epoch:  1\n",
      "736/800.0 loss: 0.21756592576911363 \n",
      "Epoch:  1\n",
      "737/800.0 loss: 0.2173058616406307 \n",
      "Epoch:  1\n",
      "738/800.0 loss: 0.21703881122101304 \n",
      "Epoch:  1\n",
      "739/800.0 loss: 0.21677076194082967 \n",
      "Epoch:  1\n",
      "740/800.0 loss: 0.21652716887152065 \n",
      "Epoch:  1\n",
      "741/800.0 loss: 0.21628340967049736 \n",
      "Epoch:  1\n",
      "742/800.0 loss: 0.21603996564620356 \n",
      "Epoch:  1\n",
      "743/800.0 loss: 0.21578181411580294 \n",
      "Epoch:  1\n",
      "744/800.0 loss: 0.21551788026754487 \n",
      "Epoch:  1\n",
      "745/800.0 loss: 0.21526966865696312 \n",
      "Epoch:  1\n",
      "746/800.0 loss: 0.21500761743879301 \n",
      "Epoch:  1\n",
      "747/800.0 loss: 0.21474461524901703 \n",
      "Epoch:  1\n",
      "748/800.0 loss: 0.21450661985733321 \n",
      "Epoch:  1\n",
      "749/800.0 loss: 0.2142570168649157 \n",
      "Epoch:  1\n",
      "750/800.0 loss: 0.21401148907016818 \n",
      "Epoch:  1\n",
      "751/800.0 loss: 0.21375584846332749 \n",
      "Epoch:  1\n",
      "752/800.0 loss: 0.21351566337108374 \n",
      "Epoch:  1\n",
      "753/800.0 loss: 0.213289609215956 \n",
      "Epoch:  1\n",
      "754/800.0 loss: 0.21302893706998288 \n",
      "Epoch:  1\n",
      "755/800.0 loss: 0.21278744154220458 \n",
      "Epoch:  1\n",
      "756/800.0 loss: 0.21254195046058735 \n",
      "Epoch:  1\n",
      "757/800.0 loss: 0.21230034139203915 \n",
      "Epoch:  1\n",
      "758/800.0 loss: 0.2120487375383674 \n",
      "Epoch:  1\n",
      "759/800.0 loss: 0.21179228393164903 \n",
      "Epoch:  1\n",
      "760/800.0 loss: 0.2115531334513982 \n",
      "Epoch:  1\n",
      "761/800.0 loss: 0.21131392713792602 \n",
      "Epoch:  1\n",
      "762/800.0 loss: 0.21108621221431195 \n",
      "Epoch:  1\n",
      "763/800.0 loss: 0.2108316936125462 \n",
      "Epoch:  1\n",
      "764/800.0 loss: 0.2105833607220572 \n",
      "Epoch:  1\n",
      "765/800.0 loss: 0.21034382110067665 \n",
      "Epoch:  1\n",
      "766/800.0 loss: 0.2100903754944596 \n",
      "Epoch:  1\n",
      "767/800.0 loss: 0.20984908374036118 \n",
      "Epoch:  1\n",
      "768/800.0 loss: 0.20961799744095155 \n",
      "Epoch:  1\n",
      "769/800.0 loss: 0.20938395208546093 \n",
      "Epoch:  1\n",
      "770/800.0 loss: 0.20915079030574296 \n",
      "Epoch:  1\n",
      "771/800.0 loss: 0.20890608930072405 \n",
      "Epoch:  1\n",
      "772/800.0 loss: 0.2086594460283877 \n",
      "Epoch:  1\n",
      "773/800.0 loss: 0.20842843747502843 \n",
      "Epoch:  1\n",
      "774/800.0 loss: 0.2081978895803613 \n",
      "Epoch:  1\n",
      "775/800.0 loss: 0.20796257177050964 \n",
      "Epoch:  1\n",
      "776/800.0 loss: 0.20773012340883873 \n",
      "Epoch:  1\n",
      "777/800.0 loss: 0.207488709835666 \n",
      "Epoch:  1\n",
      "778/800.0 loss: 0.2072471974773837 \n",
      "Epoch:  1\n",
      "779/800.0 loss: 0.20701481674630673 \n",
      "Epoch:  1\n",
      "780/800.0 loss: 0.20678061391399857 \n",
      "Epoch:  1\n",
      "781/800.0 loss: 0.20655779228033616 \n",
      "Epoch:  1\n",
      "782/800.0 loss: 0.2063216116721831 \n",
      "Epoch:  1\n",
      "783/800.0 loss: 0.20609284629237515 \n",
      "Epoch:  1\n",
      "784/800.0 loss: 0.20588435996917023 \n",
      "Epoch:  1\n",
      "785/800.0 loss: 0.20568349671433975 \n",
      "Epoch:  1\n",
      "786/800.0 loss: 0.20544381496014863 \n",
      "Epoch:  1\n",
      "787/800.0 loss: 0.20520826912045403 \n",
      "Epoch:  1\n",
      "788/800.0 loss: 0.20496923241089127 \n",
      "Epoch:  1\n",
      "789/800.0 loss: 0.20472822777714722 \n",
      "Epoch:  1\n",
      "790/800.0 loss: 0.20450570970401638 \n",
      "Epoch:  1\n",
      "791/800.0 loss: 0.2042816644099381 \n",
      "Epoch:  1\n",
      "792/800.0 loss: 0.20405723697483577 \n",
      "Epoch:  1\n",
      "793/800.0 loss: 0.20382896253735916 \n",
      "Epoch:  1\n",
      "794/800.0 loss: 0.20359547107404322 \n",
      "Epoch:  1\n",
      "795/800.0 loss: 0.2033657523857233 \n",
      "Epoch:  1\n",
      "796/800.0 loss: 0.2031546689703163 \n",
      "Epoch:  1\n",
      "797/800.0 loss: 0.20293142532061478 \n",
      "Epoch:  1\n",
      "798/800.0 loss: 0.20269481708366746 \n",
      "Epoch:  1\n",
      "799/800.0 loss: 0.2024652223382145 \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "bert_clf = BertBinaryClassifier()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       117\n",
      "           1       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss() #binomial cross entropy loss\n",
    "        loss = loss_func(logits, labels) \n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "        \n",
    "print(classification_report(test_y, bert_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECISION = TRUE POS / TRUE POS + FALSE POS\n",
    "RECALL = TRUE POS / TRUE POS + FALSE NEG\n",
    "F1 = (2 * RECALL * PRECISION) / ( RECALL + PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
